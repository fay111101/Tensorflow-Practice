{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset api\n",
    "### tensorflow 2.0 api\n",
    "* https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=tf.data.TextLineDataset([\"file1.txt\",\"file2.txt\"])\n",
    "dataset=tf.data.TFRecordDataset([\"file1.tfrecords\",\"file2.tfrecords\"])\n",
    "# dataset=tf.data.Dataset.list_files(\"/path/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.map(lambda x: x*2)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "a = 1 # Integer element\n",
    "b = 2.0 # Float element\n",
    "c = (1, 2) # Tuple element with 2 components\n",
    "d = {\"a\": (2, 2), \"b\": 3} # Dict element with 3 components\n",
    "Point = collections.namedtuple(\"Point\", [\"x\", \"y\"]) # doctest: +SKIP\n",
    "e = Point(1, 2) # Named tuple # doctest: +SKIP\n",
    "f = tf.data.Dataset.range(10) # Dataset element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.int32, name=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "def dataset_fn(ds):\n",
    "    return ds.filter(lambda x: x < 5)\n",
    "dataset = dataset.apply(dataset_fn)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
    "                                              'b': [5, 6]})\n",
    "list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
    "                                      {'a': (2, 4), 'b': 6}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.batch(3)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "#drop_remainder A tf.bool scalar tf.Tensor, representing whether the last batch should be dropped in the case it has fewer than batch_size elements; the default behavior is not to drop the smaller batch.\n",
    "dataset = dataset.batch(3, drop_remainder=True)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "dataset = dataset.map(lambda x: x**2)\n",
    "dataset = dataset.cache()\n",
    "# The first time reading through the data will generate the data using\n",
    "# `range` and `map`.\n",
    "list(dataset.as_numpy_iterator())\n",
    "\n",
    "# Subsequent iterations read from the cache.\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(5)\n",
    "dataset = dataset.cache(\"./file\")  # doctest: +SKIP\n",
    "list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
    "# [0,1,2,3,4]\n",
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset = dataset.cache(\"./file\")  # Same file! # doctest: +SKIP\n",
    "list(dataset.as_numpy_iterator())  # doctest: +SKIP\n",
    "# [0,1,2,3,4]\n",
    "# If you wish to randomize the iteration order, make sure to call shuffle after calling cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
    "b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
    "ds = a.concatenate(b)\n",
    "list(ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Two datasets to concatenate have different types <dtype: 'int64'> and (tf.int64, tf.int64)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7fd1751c8697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# nested structures and output types.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1027\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \"\"\"\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate)\u001b[0m\n\u001b[1;32m   3511\u001b[0m       raise TypeError(\n\u001b[1;32m   3512\u001b[0m           \u001b[0;34m\"Two datasets to concatenate have different types %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m           (output_types, get_legacy_output_types(dataset_to_concatenate)))\n\u001b[0m\u001b[1;32m   3514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0moutput_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_legacy_output_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Two datasets to concatenate have different types <dtype: 'int64'> and (tf.int64, tf.int64)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# The input dataset and dataset to be concatenated should have the same\n",
    "# nested structures and output types.\n",
    "c = tf.data.Dataset.zip((a, b))\n",
    "a.concatenate(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((), ()), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.data.Dataset.range(1, 5)  \n",
    "b = tf.data.Dataset.range(4, 8)\n",
    "c=tf.data.Dataset.zip((a,b))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Two datasets to concatenate have different types <dtype: 'int64'> and <dtype: 'string'>",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-cf0d6b286065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m   1027\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \"\"\"\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, dataset_to_concatenate)\u001b[0m\n\u001b[1;32m   3511\u001b[0m       raise TypeError(\n\u001b[1;32m   3512\u001b[0m           \u001b[0;34m\"Two datasets to concatenate have different types %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m           (output_types, get_legacy_output_types(dataset_to_concatenate)))\n\u001b[0m\u001b[1;32m   3514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0moutput_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_legacy_output_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Two datasets to concatenate have different types <dtype: 'int64'> and <dtype: 'string'>"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
    "a.concatenate(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "(6, 2)\n",
      "(7, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset=tf.data.Dataset.from_tensor_slices([1,2,3])\n",
    "dataset=dataset.enumerate(start=5)\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([7, 8], dtype=int32))\n",
      "(1, array([ 9, 10], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "# The nested structure of the input dataset determines the structure of\n",
    "# elements in the resulting dataset.\n",
    "dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
    "dataset = dataset.enumerate()\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "dataset = dataset.filter(lambda x: x < 3)\n",
    "list(dataset.as_numpy_iterator())\n",
    "\n",
    "# `tf.math.equal(x, y)` is required for equality comparison\n",
    "def filter_fn(x):\n",
    "    return tf.math.equal(x, 1)\n",
    "dataset = dataset.filter(filter_fn)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# from tensorflow.data import *\n",
    "dataset = dataset.flat_map(lambda x:tf.data.Dataset.from_tensor_slices(x))\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "dataset = dataset.map(lambda x: x + 1)\n",
    "list(dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.range(5)\n",
    "# `map_func` takes a single argument of type `tf.Tensor` with the same\n",
    "# shape and dtype.\n",
    "result = dataset.map(lambda x: x + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each element is a tuple containing two `tf.Tensor` objects.\n",
    "elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: elements, (tf.int32, tf.string))\n",
    "# `map_func` takes two arguments of type `tf.Tensor`. This function\n",
    "# projects out just the first component.\n",
    "result = dataset.map(lambda x_int, y_str: x_int)\n",
    "list(result.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
    "elements =  ([{\"a\": 1, \"b\": \"foo\"},\n",
    "              {\"a\": 2, \"b\": \"bar\"},\n",
    "              {\"a\": 3, \"b\": \"baz\"}])\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n",
    "# `map_func` takes a single argument of type `dict` with the same keys\n",
    "# as the elements.\n",
    "result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: <unknown>, types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Python primitives, lists, and NumPy arrays are implicitly converted to `tf.Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n",
       " TensorSpec(shape=(), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.range(3)\n",
    "# `map_func` returns two `tf.Tensor` objects.\n",
    "def g(x):\n",
    "    return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
    "result = dataset.map(g)\n",
    "result.element_spec\n",
    "\n",
    "# Python primitives, lists, and NumPy arrays are implicitly converted to\n",
    "# `tf.Tensor`.\n",
    "def h(x):\n",
    "    return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
    "result = dataset.map(h)\n",
    "result.element_spec\n",
    "\n",
    "# `map_func` can return nested structures.\n",
    "def i(x):\n",
    "    return (37.0, [42, 16]), \"foo\"\n",
    "result = dataset.map(i)\n",
    "result.element_spec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'HELLO', b'WORLD']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=tf.data.Dataset.from_tensor_slices(['hello','world'])\n",
    "def upper_case_fn(t:tf.Tensor):\n",
    "    return t.numpy().decode('utf-8').upper()\n",
    "d=d.map(lambda x:tf.py_function(func=upper_case_fn,\n",
    "                               inp=[x],Tout=tf.string))\n",
    "list(d.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance can often be improved by setting num_parallel_calls \n",
    "# so that map will use multiple threads to process elements. \n",
    "# If deterministic order isn't required, it can also improve performance \n",
    "# to set deterministic=False.\n",
    "dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "dataset = dataset.map(lambda x: x + 1,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset.padded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf2 例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7fdc05f6ca60> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "     .map(lambda x: tf.fill([x], x)))\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7fdc05f6ca60> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "     .map(lambda x: tf.fill([x], x)))\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[[1 0]\n",
      " [2 2]]\n",
      "[[3 3 3 0]\n",
      " [4 4 4 4]]\n"
     ]
    }
   ],
   "source": [
    "A = (tf.data.Dataset.range(1, 5, output_type=tf.int32)\n",
    "     .map(lambda x: tf.fill([x], x)))\n",
    "# Pad to the smallest per-batch size that fits all elements.\n",
    "B = A.padded_batch(2)\n",
    "for element in B.as_numpy_iterator():\n",
    "      print(element)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1], dtype=int32), array([2, 2], dtype=int32), array([3, 3, 3], dtype=int32), array([4, 4, 4, 4], dtype=int32)]\n",
      "[[1 0 0 0 0]\n",
      " [2 2 0 0 0]]\n",
      "[[3 3 3 0 0]\n",
      " [4 4 4 4 0]]\n"
     ]
    }
   ],
   "source": [
    "print(list(A.as_numpy_iterator()))\n",
    "\n",
    "# Pad to a fixed size.\n",
    "C = A.padded_batch(2, padded_shapes=5)\n",
    "for element in C.as_numpy_iterator():\n",
    "      print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1 -1 -1 -1]\n",
      " [ 2  2 -1 -1 -1]]\n",
      "[[ 3  3  3 -1 -1]\n",
      " [ 4  4  4  4 -1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Pad with a custom value.\n",
    "D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
    "for element in D.as_numpy_iterator():\n",
    "      print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 1, -1],\n",
      "       [ 2,  2]], dtype=int32), array([[ 1, -1],\n",
      "       [ 2,  2]], dtype=int32))\n",
      "(array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n",
      "       [ 4,  4,  4,  4]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Components of nested elements can be padded independently.\n",
    "elements = [([1, 2, 3], [10]),\n",
    "            ([4, 5], [11, 12])]\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: iter(elements), (tf.int32, tf.int32))\n",
    "# Pad the first component of the tuple to length 4, and the second\n",
    "# component to the smallest size that fits.\n",
    "dataset = dataset.padded_batch(2,\n",
    "    padded_shapes=([4], [None]),\n",
    "    padding_values=(-1, 100))\n",
    "list(dataset.as_numpy_iterator())\n",
    "\n",
    "\n",
    "# Pad with a single value and multiple components.\n",
    "E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
    "for element in E.as_numpy_iterator():\n",
    "    print(element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### tf1.x例子\n",
    "* https://blog.csdn.net/z2539329562/article/details/89791783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# \n",
    "# tf1.x需要手动开启eager模式\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1, 0, 0]), array([2, 3, 0]), array([4, 5, 6]), array([7, 8, 0]), array([9, 0, 0]), array([0, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    " \n",
    "x = [[1, 0, 0],\n",
    "     [2, 3, 0],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 0],\n",
    "     [9, 0, 0],\n",
    "     [0, 1, 0]]\n",
    "x_new = [np.array(i) for i in x]\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-13c6274560ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#         print(sess.run(iterator1.get_next()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.x/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow1.x/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1078\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#tf.TensorShape([])     表示长度为单个数字\n",
    "#tf.TensorShape([None]) 表示长度未知的向量\n",
    "padded_shapes=(\n",
    "        tf.TensorShape([None])\n",
    "        )\n",
    " \n",
    "#   padded_shapes=(\n",
    "#        tf.TensorShape([None]),\n",
    "#        )\n",
    "#TypeError: Expected int64, got TensorShape([Dimension(None)]) of type 'TensorShape' instead.\n",
    "# 注意，在tf.TensorShape([None])后面不能添加 \",\",因为这里递归嵌套，会认为\",\"后面还有一维数据，\n",
    "# 只是数据格式为 None。\n",
    " \n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "iterator1=dataset.make_one_shot_iterator()\n",
    "\n",
    "dataset = dataset.padded_batch(2, padded_shapes=padded_shapes)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "sess = tf.Session()\n",
    "try:\n",
    "    while True:\n",
    "        print(sess.run(iterator.get_next()))\n",
    "        print(\"=\"*10)\n",
    "#         print(sess.run(iterator1.get_next()))\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "\n",
    "# dataset = dataset.padded_batch(4, padded_shapes=[None])\n",
    "dataset = dataset.batch(2)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    " \n",
    "print(sess.run(next_element)) \n",
    "# print(sess.run(next_element))  \n",
    "# print(sess.run(next_element))  \n",
    "# print(sess.run(next_element))  \n",
    "# print(sess.run(next_element))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    " \n",
    "x = [[1, 0, 0],\n",
    "     [2, 3, 0],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 0],\n",
    "     [9, 0, 0],\n",
    "     [0, 1, 0]]\n",
    " \n",
    " \n",
    " \n",
    "#tf.TensorShape([])     表示长度为单个数字\n",
    "#tf.TensorShape([None]) 表示长度未知的向量\n",
    "padded_shapes=(\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([]),\n",
    "        tf.TensorShape([])\n",
    "        )\n",
    " \n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset = dataset.map(lambda x: [x[0], x[1], x[2]])\n",
    "dataset = dataset.padded_batch(2, padded_shapes=padded_shapes)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "sess = tf.Session()\n",
    "try:\n",
    "    while True:\n",
    "        elem1, elem2, elem3 = iterator.get_next()\n",
    "        print(\"elem:\", sess.run(elem1))\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
